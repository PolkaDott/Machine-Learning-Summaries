#### Метод наименьших квадратов
![[Pasted image 20220908182609.png]]

На рис. синяя линия - это максимально приближенная линия, которая описывала бы направление и характер распределения данных. Метод наименьших квадратов подразумевает расчёт коэффициента b1, который позволит задать угол прямой. Впоследствии, это может помочь грязно экстраполировать данные.

Коэффициент b0 нужен для того, чтобы понять, насколько среднее значение будет удалено от начала графика. Или же какое будет значение зависимой переменной, если все независимые будут равны нулю.

![[Pasted image 20220908182622.png]]

Если есть необходимость проверить гипотезу о том, что b1 = 0 (данные не имеют взаимосвязи), то используют t-критерий как на рис. выше (Т-тест. T-распределение помогает получить значение p-value).
#### Коэффициент детерминации

![[Pasted image 20220908182635.png]]

Коэффициент детерминации - грубо говоря дисперсия вокруг прямой, которая характеризует корреляцию. Если он большой, значит значения очень близко лежат к прямой и данные ведут себя довольно предсказуемо. В обратном случае, если коэффициент равен нулю, то данные хаотичные и почти не взаимосвязаны. Поэтому он и называется квадратом коэффициента корреляции. Если R2 = 0,95, можно сказать, что 95 % изменчивости переменной объясняется нашей моделью.

![[Pasted image 20220908182642.png]]

Данные также следовало бы сначала проверить на несколько параметров. На рис. пример неправильных данных, но вполне нормальных коэффициентов корреляции. Сами параметры:

-        Линейная зависимость переменных
-        Нормальное распределение остатков
-        Гетероскедастичность
-        Проверка на мультиколлинеарность (в случае с множественной регрессией)
-        Нормальное распределение переменных (желательно)
Также есть множественная регрессия. Она позволяет проанализировать сразу несколько независимых переменных и их влияние на зависимую.
![[Pasted image 20220908182701.png]]
![](file:///C:/Users/Polka/AppData/Local/Temp/msohtmlclip1/01/clip_image006.gif)Анализ данных с рисунка: коэффициент выборки metro_res статистически значим (p = 0.006), можно сказать, что при неизменчивости других параметров, зависимая переменная будет расти на -0.06. Коэффициент детерминации (R2) как всегда помогает определить зависимость отклонения зав.переменной от независимых. Но в случае с множественной регрессией, нужно использовать исправленный коэффициент детерминации, потому что с ростом количества переменных растёт шанс найти случайные корреляции.

#### Построение модели

Также необходимо отсеять ненужные переменные, которые могут понижать коэффициент детерминации (R2).

![](file:///C:/Users/Polka/AppData/Local/Temp/msohtmlclip1/01/clip_image008.gif)
![[Pasted image 20220908182714.png]]
На рис. видно, что переменная female_house очень сильно коррелирует со всеми другими переменными. Существует рофл, что _если независимые переменные сильно между собой коррелируют - это плохо_. Если её убрать, то значение R2 будет лучше, поэтому от нее следует избавиться. (Это, кажется, и называется **_МУЛЬТИКОЛЛИНЕАРНОСТЬ_**).

Можно разбить на этапы построение итоговой регрессионной модели (отсебятина):
-        Смотрим на графиках, подходят ли наши данные для регрессионного анализа
-        Изучаем зависимости
-        Убираем лишние переменные
-        Выводим общую формулу, характеризующую регрессионную модель (рис. ниже)
-        Смотрим опять же адекватность модели по графикам (например QQ Plot, Анализ остатков..)

![[Pasted image 20220908182723.png]]
![](file:///C:/Users/Polka/AppData/Local/Temp/msohtmlclip1/01/clip_image010.gif)

Мы можем использовать _логистическую регрессию_, если у нас зависимая переменная номинативная (0 или 1).