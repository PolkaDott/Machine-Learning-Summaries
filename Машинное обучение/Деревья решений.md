### По методу энтропии  
Энтропия - это мера определённости (в случае с деревьями решений). Абсолютная неопределённость и полная определённость соответственно 1 и 0  
$$E=-\sum_{i=1}^{n} p_i\cdot\log_2(p_i)$$  
В формуле выше n - это количество вариантов результата, а p - вероятность каждого из них. Например, данные, где 4 котика и 6 собачек:  
$$E=-\frac{4}{10}\cdot\log_2\frac{4}{10}-\frac{6}{10}\cdot\log_2\frac{6}{10}=0.971$$  
Суть метода состоит в том, что необходимо на каждую *"фичу"* (предиктор, переменная, столбик в data frame) последовательно вычислить их энтропии, разбить по этой фиче данные на *сплит*, в котором далее вычислять энтропии и разбивать данные на более мелкие  
![Pasted image 20221124161328.png|450](https://github.com/PolkaDott/Data-Science-Summaries/blob/main/Машинное%20обучение/attachments/Pasted%20image%2020221124161328.png?raw=true)  
На каждом уровне вычисляется вычисляются энтропия для всех фичей и применяется самая маленькая  
Также есть понятие *Information Gain* - мера снижения неопределённости между уровнями. Нужно для оценки разбиения  
$$IG=E(Y)-E(Y/X)$$  
В формуле выше $E(Y)$ - это изначальная энтропия, а $E(Y/X)$ - энтропия при разбиении по фиче $X$  
